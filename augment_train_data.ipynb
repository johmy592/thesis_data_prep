{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from copy import deepcopy\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trucks Wiki:\n",
    "#terms_vocab = \"/home/johannes/thesis_code/data_experimentation/trucks/term_vocab.txt\"\n",
    "#full_vocab = \"/home/johannes/thesis_code/data_experimentation/vocab_concat/vocabulary.txt\"\n",
    "\n",
    "# Volvo manual\n",
    "terms_vocab = \"/home/johannes/thesis_code/data_experimentation/volvo_data/term_vocab.txt\"\n",
    "full_vocab = \"/home/johannes/thesis_code/data_experimentation/volvo_data/vocabulary.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5346\n"
     ]
    }
   ],
   "source": [
    "tf = open(terms_vocab,'r')\n",
    "terms_list = [w.strip('\\n') for w in tf]\n",
    "tf.close()\n",
    "print(len(terms_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_synsets(terms):\n",
    "    '''\n",
    "    Returns a list with all terms that have a wordnet synset\n",
    "    '''\n",
    "    return [w for w in terms if wn.synsets(w.replace(' ','_'))]\n",
    "\n",
    "def select_synsets(terms_with_synsets):\n",
    "    '''\n",
    "    Returns a dictionary with first available synset with\n",
    "    a noun POS tag\n",
    "    '''\n",
    "    synset_dict = {}\n",
    "    for t in terms_with_synsets:\n",
    "        ss = wn.synsets(t.replace(' ','_'))\n",
    "        for s in ss:\n",
    "            if '.n' in s.name():\n",
    "                synset_dict[t] = s\n",
    "                break\n",
    "    return synset_dict\n",
    "        \n",
    "def make_hypernym_dict(synset_dict,num_levels=2):\n",
    "    '''\n",
    "    Returns a dictionary of terms associated with\n",
    "    its hypernym synsets, traverses num_levels levels\n",
    "    in the wordnet hierarchy\n",
    "    '''\n",
    "    hypernym_dict = {}\n",
    "    for term in synset_dict:\n",
    "        cur_level = 0\n",
    "        next_hypernyms = synset_dict[term].hypernyms()\n",
    "        \n",
    "        hypernym_dict[term] = next_hypernyms\n",
    "        cur_level += 1\n",
    "        while cur_level < num_levels:\n",
    "            _next_hypernyms = []\n",
    "            for h in next_hypernyms:\n",
    "                _next_hypernyms += h.hypernyms()\n",
    "            hypernym_dict[term] += _next_hypernyms\n",
    "            next_hypernyms = _next_hypernyms\n",
    "            cur_level += 1\n",
    "            \n",
    "    return hypernym_dict\n",
    "\n",
    "def keep_vocab_hypernyms(hypernym_dict, full_vocab_list):\n",
    "    '''\n",
    "    Removes hypernyms that do not exist in the\n",
    "    vocabulary\n",
    "    '''\n",
    "    num_found = 0\n",
    "    processed_terms = 0\n",
    "    new_dict = {}\n",
    "    for term in hypernym_dict:\n",
    "        hypernyms_to_keep = []\n",
    "        for h in hypernym_dict[term]:\n",
    "            hypernym_text = h.name().split('.n')[0].replace('_',' ') \n",
    "            if hypernym_text in full_vocab_list:\n",
    "                hypernyms_to_keep += [hypernym_text]\n",
    "        if hypernyms_to_keep:\n",
    "            new_dict[term] = hypernyms_to_keep\n",
    "        processed_terms += 1\n",
    "        if processed_terms%100 == 0:\n",
    "            print(\"Processed \", str(processed_terms), \" terms\")\n",
    "    return new_dict\n",
    "        \n",
    "    \n",
    "def sample_terms(hypernym_dict, num_samples=100):\n",
    "    '''\n",
    "    Returns a random sample of num_samples terms\n",
    "    '''\n",
    "    all_terms = [t for t in hypernym_dict]\n",
    "    return random.sample(all_terms, num_samples)\n",
    "\n",
    "def write_training_data(queries, hypernym_dict, query_file, gold_file):\n",
    "    '''\n",
    "    Writes a query file and a gold file in accordance with the \n",
    "    SemEval-2018 Task 9 standard\n",
    "    '''\n",
    "    qf = open(query_file,'w+')\n",
    "    gf = open(gold_file,'w+')\n",
    "    for q in queries:\n",
    "        qf.write(q)\n",
    "        qf.write('\\t')\n",
    "        qf.write('Concept')\n",
    "        qf.write('\\n')\n",
    "        unique_hypernyms = list(set(hypernym_dict[q]))\n",
    "        for i in range(len(unique_hypernyms)):\n",
    "            gf.write(unique_hypernyms[i])\n",
    "            if not i == (len(unique_hypernyms)-1):\n",
    "                gf.write('\\t')\n",
    "        gf.write('\\n')\n",
    "    qf.close()\n",
    "    gf.close()\n",
    "    \n",
    "def sanity_check(hypernym_dict, full_vocab_list):\n",
    "    '''\n",
    "    Performs a sanity check to make sure that no \n",
    "    out-of-vocabulary terms remain\n",
    "    '''\n",
    "    for term in hypernym_dict:\n",
    "        if term not in full_vocab_list:\n",
    "            print(\"Found out of vocab term: \", term ,\" something went wrong!\")\n",
    "            return\n",
    "        for h in hypernym_dict[term]:\n",
    "            if h not in full_vocab_list:\n",
    "                print(\"Found out of vocab term: \", h , \" something went wrong!\")\n",
    "                return\n",
    "    print(\"All good!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534\n"
     ]
    }
   ],
   "source": [
    "terms_with_synsets = check_synsets(terms_list)\n",
    "print(len(terms_with_synsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time signal Synset('time_signal.n.01') [Synset('signal.n.01')]\n",
      "fuel gauge Synset('fuel_gauge.n.01') [Synset('indicator.n.03')]\n"
     ]
    }
   ],
   "source": [
    "term_synset_dict = select_synsets(terms_with_synsets)\n",
    "print_num =2\n",
    "for key in term_synset_dict:\n",
    "    print(key, term_synset_dict[key], term_synset_dict[key].hypernyms())\n",
    "    print_num -= 1\n",
    "    if not print_num:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernym_dict = make_hypernym_dict(term_synset_dict,num_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuel gauge [Synset('indicator.n.03'), Synset('device.n.01'), Synset('instrumentality.n.03')]\n",
      "merchantability [Synset('state.n.02'), Synset('attribute.n.02'), Synset('abstraction.n.06')]\n"
     ]
    }
   ],
   "source": [
    "print_num = 2\n",
    "for t in hypernym_dict:\n",
    "    print(t,hypernym_dict[t])\n",
    "    print_num -= 1\n",
    "    if not print_num:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223407\n"
     ]
    }
   ],
   "source": [
    "fvf = open(full_vocab,'r')\n",
    "full_vocab_list = [w.strip('\\n') for w in fvf]\n",
    "fvf.close()\n",
    "print(len(full_vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  100  terms\n",
      "Processed  200  terms\n",
      "Processed  300  terms\n",
      "Processed  400  terms\n",
      "Processed  500  terms\n"
     ]
    }
   ],
   "source": [
    "ready_hypernyms = keep_vocab_hypernyms(hypernym_dict, full_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time signal ['signal']\n",
      "fuel gauge ['indicator']\n",
      "merchantability ['state']\n",
      "federal communications commission ['independent agency']\n",
      "seal ring ['ring', 'jewelry', 'adornment']\n"
     ]
    }
   ],
   "source": [
    "print_num = 5\n",
    "for t in ready_hypernyms:\n",
    "    print(t, ready_hypernyms[t])\n",
    "    print_num -= 1\n",
    "    if not print_num:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = sample_terms(ready_hypernyms,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trucks wiki\n",
    "#query_file = \"/home/johannes/thesis_code/data_experimentation/new_training_data/queries.txt\"\n",
    "#gold_file = \"/home/johannes/thesis_code/data_experimentation/new_training_data/gold.txt\"\n",
    "\n",
    "# Volvo manual\n",
    "query_file = \"/home/johannes/thesis_code/data_experimentation/volvo_training_data/queries.txt\"\n",
    "gold_file = \"/home/johannes/thesis_code/data_experimentation/volvo_training_data/gold.txt\"\n",
    "\n",
    "write_training_data(queries, ready_hypernyms, query_file, gold_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_check(ready_hypernyms, full_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_training_data(qf1, qf2, gf1, gf2, write_q, write_g):\n",
    "    _wq = open(write_q,'w+')\n",
    "    _qf1 = open(qf1,'r')\n",
    "    for line in _qf1:\n",
    "        _wq.write(line)\n",
    "    _qf1.close()\n",
    "    _wq.write('\\n')\n",
    "    _qf2 = open(qf2,'r')\n",
    "    for line in _qf2:\n",
    "        _wq.write(line)\n",
    "    _qf2.close()\n",
    "    _wq.close()\n",
    "    \n",
    "    _wg = open(write_g,'w+')\n",
    "    _gf1 = open(gf1,'r')\n",
    "    for line in _gf1:\n",
    "        _wg.write(line)\n",
    "    _gf1.close()\n",
    "    _gf2 = open(gf2,'r')\n",
    "    _wg.write('\\n')\n",
    "    for line in _gf2:\n",
    "        _wg.write(line)\n",
    "    _gf2.close()\n",
    "    _wg.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"/home/johannes/hypernym_discovery_data/SemEval2018-Task9/training/data/1A.english.training.data.txt\"\n",
    "q2 = \"/home/johannes/thesis_code/data_experimentation/volvo_training_data/queries.txt\"\n",
    "g1 = \"/home/johannes/hypernym_discovery_data/SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
    "g2 = \"/home/johannes/thesis_code/data_experimentation/volvo_training_data/gold.txt\"\n",
    "write_queries = \"/home/johannes/thesis_code/data_experimentation/volvo_training_data/concat_queries.txt\"\n",
    "write_gold = \"/home/johannes/thesis_code/data_experimentation/volvo_training_data/concat_gold.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_training_data(q1, q2, g1, g2, write_queries, write_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
