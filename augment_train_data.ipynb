{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from copy import deepcopy\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_vocab = \"/home/johannes/thesis_code/data_experimentation/trucks/term_vocab.txt\"\n",
    "full_vocab = \"/home/johannes/thesis_code/data_experimentation/vocab_concat/vocabulary.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72184\n"
     ]
    }
   ],
   "source": [
    "tf = open(terms_vocab,'r')\n",
    "terms_list = [w.strip('\\n') for w in tf]\n",
    "tf.close()\n",
    "print(len(terms_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_synsets(terms):\n",
    "    '''\n",
    "    Returns a list with all terms that have a wordnet synset\n",
    "    '''\n",
    "    return [w for w in terms if wn.synsets(w.replace(' ','_'))]\n",
    "\n",
    "def select_synsets(terms_with_synsets):\n",
    "    '''\n",
    "    Returns a dictionary with first available synset with\n",
    "    a noun POS tag\n",
    "    '''\n",
    "    synset_dict = {}\n",
    "    for t in terms_with_synsets:\n",
    "        ss = wn.synsets(t.replace(' ','_'))\n",
    "        for s in ss:\n",
    "            if '.n' in s.name():\n",
    "                synset_dict[t] = s\n",
    "                break\n",
    "    return synset_dict\n",
    "        \n",
    "def make_hypernym_dict(synset_dict,num_levels=2):\n",
    "    '''\n",
    "    Returns a dictionary of terms associated with\n",
    "    its hypernym synsets, traverses num_levels levels\n",
    "    in the wordnet hierarchy\n",
    "    '''\n",
    "    hypernym_dict = {}\n",
    "    for term in synset_dict:\n",
    "        cur_level = 0\n",
    "        next_hypernyms = synset_dict[term].hypernyms()\n",
    "        \n",
    "        hypernym_dict[term] = next_hypernyms\n",
    "        cur_level += 1\n",
    "        while cur_level < num_levels:\n",
    "            _next_hypernyms = []\n",
    "            for h in next_hypernyms:\n",
    "                _next_hypernyms += h.hypernyms()\n",
    "            hypernym_dict[term] += _next_hypernyms\n",
    "            next_hypernyms = _next_hypernyms\n",
    "            cur_level += 1\n",
    "            \n",
    "    return hypernym_dict\n",
    "\n",
    "def keep_vocab_hypernyms(hypernym_dict, full_vocab_list):\n",
    "    '''\n",
    "    Removes hypernyms that do not exist in the\n",
    "    vocabulary\n",
    "    '''\n",
    "    num_found = 0\n",
    "    processed_terms = 0\n",
    "    new_dict = {}\n",
    "    for term in hypernym_dict:\n",
    "        hypernyms_to_keep = []\n",
    "        for h in hypernym_dict[term]:\n",
    "            hypernym_text = h.name().split('.n')[0].replace('_',' ') \n",
    "            if hypernym_text in full_vocab_list:\n",
    "                hypernyms_to_keep += [hypernym_text]\n",
    "        if hypernyms_to_keep:\n",
    "            new_dict[term] = hypernyms_to_keep\n",
    "        processed_terms += 1\n",
    "        if processed_terms%100 == 0:\n",
    "            print(\"Processed \", str(processed_terms), \" terms\")\n",
    "    return new_dict\n",
    "        \n",
    "    \n",
    "def sample_terms(hypernym_dict, num_samples=100):\n",
    "    '''\n",
    "    Returns a random sample of num_samples terms\n",
    "    '''\n",
    "    all_terms = [t for t in hypernym_dict]\n",
    "    return random.sample(all_terms, num_samples)\n",
    "\n",
    "def write_training_data(queries, hypernym_dict, query_file, gold_file):\n",
    "    '''\n",
    "    Writes a query file and a gold file in accordance with the \n",
    "    SemEval-2018 Task 9 standard\n",
    "    '''\n",
    "    qf = open(query_file,'w+')\n",
    "    gf = open(gold_file,'w+')\n",
    "    for q in queries:\n",
    "        qf.write(q)\n",
    "        qf.write('\\n')\n",
    "        unique_hypernyms = list(set(hypernym_dict[q]))\n",
    "        for i in range(len(unique_hypernyms)):\n",
    "            gf.write(unique_hypernyms[i])\n",
    "            if not i == (len(unique_hypernyms)-1):\n",
    "                gf.write('\\t')\n",
    "        gf.write('\\n')\n",
    "    qf.close()\n",
    "    gf.close()\n",
    "    \n",
    "def sanity_check(hypernym_dict, full_vocab_list):\n",
    "    '''\n",
    "    Performs a sanity check to make sure that no \n",
    "    out-of-vocabulary terms remain\n",
    "    '''\n",
    "    for term in hypernym_dict:\n",
    "        if term not in full_vocab_list:\n",
    "            print(\"Found out of vocab term: \", term ,\" something went wrong!\")\n",
    "            return\n",
    "        for h in hypernym_dict[term]:\n",
    "            if h not in full_vocab_list:\n",
    "                print(\"Found out of vocab term: \", h , \" something went wrong!\")\n",
    "                return\n",
    "    print(\"All good!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6953\n"
     ]
    }
   ],
   "source": [
    "terms_with_synsets = check_synsets(terms_list)\n",
    "print(len(terms_with_synsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccs Synset('milliliter.n.01') [Synset('metric_capacity_unit.n.01')]\n",
      "biomass Synset('biomass.n.01') [Synset('fuel.n.01')]\n"
     ]
    }
   ],
   "source": [
    "term_synset_dict = select_synsets(terms_with_synsets)\n",
    "print_num =2\n",
    "for key in term_synset_dict:\n",
    "    print(key, term_synset_dict[key], term_synset_dict[key].hypernyms())\n",
    "    print_num -= 1\n",
    "    if not print_num:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernym_dict = make_hypernym_dict(term_synset_dict,num_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccs [Synset('metric_capacity_unit.n.01'), Synset('metric_unit.n.01'), Synset('volume_unit.n.01'), Synset('unit_of_measurement.n.01'), Synset('unit_of_measurement.n.01')]\n",
      "modernisation [Synset('improvement.n.02'), Synset('change_of_state.n.01'), Synset('change.n.03')]\n"
     ]
    }
   ],
   "source": [
    "print_num = 2\n",
    "for t in hypernym_dict:\n",
    "    print(t,hypernym_dict[t])\n",
    "    print_num -= 1\n",
    "    if not print_num:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281828\n"
     ]
    }
   ],
   "source": [
    "fvf = open(full_vocab,'r')\n",
    "full_vocab_list = [w.strip('\\n') for w in fvf]\n",
    "fvf.close()\n",
    "print(len(full_vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  100  terms\n",
      "Processed  200  terms\n",
      "Processed  300  terms\n",
      "Processed  400  terms\n",
      "Processed  500  terms\n",
      "Processed  600  terms\n",
      "Processed  700  terms\n",
      "Processed  800  terms\n",
      "Processed  900  terms\n",
      "Processed  1000  terms\n",
      "Processed  1100  terms\n",
      "Processed  1200  terms\n",
      "Processed  1300  terms\n",
      "Processed  1400  terms\n",
      "Processed  1500  terms\n",
      "Processed  1600  terms\n",
      "Processed  1700  terms\n",
      "Processed  1800  terms\n",
      "Processed  1900  terms\n",
      "Processed  2000  terms\n",
      "Processed  2100  terms\n",
      "Processed  2200  terms\n",
      "Processed  2300  terms\n",
      "Processed  2400  terms\n",
      "Processed  2500  terms\n",
      "Processed  2600  terms\n",
      "Processed  2700  terms\n",
      "Processed  2800  terms\n",
      "Processed  2900  terms\n",
      "Processed  3000  terms\n",
      "Processed  3100  terms\n",
      "Processed  3200  terms\n",
      "Processed  3300  terms\n",
      "Processed  3400  terms\n",
      "Processed  3500  terms\n",
      "Processed  3600  terms\n",
      "Processed  3700  terms\n",
      "Processed  3800  terms\n",
      "Processed  3900  terms\n",
      "Processed  4000  terms\n",
      "Processed  4100  terms\n",
      "Processed  4200  terms\n",
      "Processed  4300  terms\n",
      "Processed  4400  terms\n",
      "Processed  4500  terms\n",
      "Processed  4600  terms\n",
      "Processed  4700  terms\n",
      "Processed  4800  terms\n",
      "Processed  4900  terms\n",
      "Processed  5000  terms\n",
      "Processed  5100  terms\n",
      "Processed  5200  terms\n",
      "Processed  5300  terms\n",
      "Processed  5400  terms\n",
      "Processed  5500  terms\n",
      "Processed  5600  terms\n",
      "Processed  5700  terms\n",
      "Processed  5800  terms\n",
      "Processed  5900  terms\n",
      "Processed  6000  terms\n",
      "Processed  6100  terms\n",
      "Processed  6200  terms\n",
      "Processed  6300  terms\n",
      "Processed  6400  terms\n",
      "Processed  6500  terms\n",
      "Processed  6600  terms\n"
     ]
    }
   ],
   "source": [
    "ready_hypernyms = keep_vocab_hypernyms(hypernym_dict, full_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccs ['metric unit', 'volume unit']\n",
      "biomass ['fuel']\n",
      "bathrobe ['robe', 'clothing']\n",
      "pitch accent ['stress', 'prosody', 'manner of speaking']\n",
      "local ['public transport', 'conveyance']\n"
     ]
    }
   ],
   "source": [
    "print_num = 5\n",
    "for t in ready_hypernyms:\n",
    "    print(t, ready_hypernyms[t])\n",
    "    print_num -= 1\n",
    "    if not print_num:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = sample_terms(ready_hypernyms,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = \"/home/johannes/thesis_code/data_experimentation/new_training_data/queries.txt\"\n",
    "gold_file = \"/home/johannes/thesis_code/data_experimentation/new_training_data/gold.txt\"\n",
    "write_training_data(queries, ready_hypernyms, query_file, gold_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_check(ready_hypernyms, full_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
