{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['tagging','parser','ner'])\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "texts_dir = \"/home/johannes/hypernym_discovery_data/fodina/files_for_exjobb_2019-02-04/extracted_text_trucks/\"\n",
    "save_location = \"/home/johannes/thesis_code/data_experimentation/tokenized_texts/tokenized_truck.txt\"\n",
    "\n",
    "IGNORE_TOKENS = ['༎','༒']\n",
    "IGNORE_WHOLE_LINE = ['༔','\\\\']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(files_dir):\n",
    "    return [f for f in listdir(files_dir) if isfile(join(files_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = get_filenames(texts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(filename):\n",
    "    tf = open(filename,'r')\n",
    "    texts_list = []\n",
    "    for line in tf:\n",
    "        ignore_line=False\n",
    "        for ic in IGNORE_WHOLE_LINE:\n",
    "            if ic in line:\n",
    "                ignore_line=True\n",
    "        if ignore_line:\n",
    "            continue\n",
    "        clean_line = \"\"\n",
    "        for c in line:\n",
    "            if c in IGNORE_TOKENS:\n",
    "                if clean_line.strip('\\n').strip(' '):\n",
    "                    texts_list.append(clean_line.replace(u'\\xa0', u' ').strip('\\n'))\n",
    "                    clean_line=\"\"\n",
    "                continue\n",
    "            clean_line += c\n",
    "        if clean_line.strip('\\n').strip(' '):\n",
    "            texts_list.append(clean_line.replace(u'\\xa0', u' ').strip('\\n'))\n",
    "    tf.close()\n",
    "    return texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenized_doc_list(texts_list):\n",
    "    \n",
    "    \n",
    "    #return [tokenizer(line) for line in texts_list]\n",
    "    return [nlp(line) for line in texts_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokenized_text_to_file(doc_list, file_path):\n",
    "    fp = open(file_path,'w+')\n",
    "    for doc in doc_list:\n",
    "        sentences = doc.sents\n",
    "        #if len(list(sentences)) > 1:\n",
    "        for s in sentences:\n",
    "            for t in s:\n",
    "                if t.text == ' ':\n",
    "                    continue\n",
    "                fp.write(t.text)\n",
    "                if t.text != '\\n':\n",
    "                    fp.write(' ')\n",
    "            fp.write('\\n')\n",
    "        #continue\n",
    "                    \n",
    "        \"\"\"for t in doc:\n",
    "            \n",
    "            if t.text == ' ':\n",
    "                continue\n",
    "            fp.write(t.text)\n",
    "            if t.text != '\\n':\n",
    "                fp.write(' ')\"\"\"\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_all(files, location):\n",
    "    all_lines = []\n",
    "    print('Extracting lines from ',len(files), 'files\\n')\n",
    "    for filename in files:\n",
    "        all_lines += extract_text(texts_dir+filename)\n",
    "    print('Tokenizing ', len(all_lines),'lines\\n')\n",
    "    docs = make_tokenized_doc_list(all_lines)\n",
    "    print('Writing to file\\n')\n",
    "    write_tokenized_text_to_file(docs, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting lines from  567 files\n",
      "\n",
      "Tokenizing  82352 lines\n",
      "\n",
      "Writing to file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregate_all(files_list,save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
