{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['tagging','parser','ner'])\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "# Trucks Wiki:\n",
    "#texts_dir = \"/home/johannes/hypernym_discovery_data/fodina/files_for_exjobb_2019-02-04/extracted_text_trucks/\"\n",
    "#save_location = \"/home/johannes/thesis_code/data_experimentation/tokenized_texts/tokenized_truck.txt\"\n",
    "\n",
    "# Volvo manual:\n",
    "texts_dir = \"/home/johannes/hypernym_discovery_data/fodina/Volvo_FH_till_Johannes_2019-03-07/extracted_text/\"\n",
    "save_location = \"/home/johannes/thesis_code/data_experimentation/volvo_data/tokenized_sentences.txt\"\n",
    "\n",
    "\n",
    "IGNORE_TOKENS = ['༔','༅','༒','༖','༗','\u000e']\n",
    "DIVIDERS = ['༎']\n",
    "IGNORE_WHOLE_LINE = ['\\\\']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tibet_chars(filename):\n",
    "    tf = open(filename,'r')\n",
    "    texts_list = []\n",
    "    for line in tf:\n",
    "        ignore_line=False\n",
    "        for ic in IGNORE_WHOLE_LINE:\n",
    "            if ic in line:\n",
    "                ignore_line=True\n",
    "        if ignore_line:\n",
    "            continue \n",
    "        for div in DIVIDERS:\n",
    "            line = line.replace(div,'\\n')\n",
    "        clean_line = line.split('\\n')\n",
    "        texts_list += [l for l in clean_line if l]\n",
    "    return texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(files_dir):\n",
    "    return [f for f in listdir(files_dir) if isfile(join(files_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = get_filenames(texts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(texts):\n",
    "    #tf = open(filename,'r')\n",
    "    texts_list = []\n",
    "    for line in texts:\n",
    "        ignore_line=False\n",
    "        for ic in IGNORE_WHOLE_LINE:\n",
    "            if ic in line:\n",
    "                ignore_line=True\n",
    "        if ignore_line:\n",
    "            continue\n",
    "        clean_line = \"\"\n",
    "        for c in line:\n",
    "            if c in IGNORE_TOKENS:\n",
    "                if clean_line.strip('\\n').strip(' '):\n",
    "                    texts_list.append(clean_line.replace(u'\\xa0', u' ').strip('\\n'))\n",
    "                    clean_line=\"\"\n",
    "                continue\n",
    "            clean_line += c\n",
    "        if clean_line.strip('\\n').strip(' '):\n",
    "            texts_list.append(clean_line.replace(u'\\xa0', u' ').strip('\\n'))\n",
    "    #tf.close()\n",
    "    return texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenized_doc_list(texts_list):\n",
    "    \n",
    "    \n",
    "    #return [tokenizer(line) for line in texts_list]\n",
    "    return [nlp(line) for line in texts_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokenized_text_to_file(doc_list, file_path):\n",
    "    fp = open(file_path,'w+')\n",
    "    for doc in doc_list:\n",
    "        sentences = doc.sents\n",
    "        #if len(list(sentences)) > 1:\n",
    "        for s in sentences:\n",
    "            line = \"\"\n",
    "            for t in s:\n",
    "                if t.text in [' ','\\t']:\n",
    "                    continue\n",
    "                line += t.text\n",
    "                if t.text != '\\n':\n",
    "                    line += ' '\n",
    "            if line.strip(' ').strip('\\n'):\n",
    "                fp.write(line)\n",
    "                fp.write('\\n')\n",
    "        #continue\n",
    "                    \n",
    "        \"\"\"for t in doc:\n",
    "            \n",
    "            if t.text == ' ':\n",
    "                continue\n",
    "            fp.write(t.text)\n",
    "            if t.text != '\\n':\n",
    "                fp.write(' ')\"\"\"\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_all(files, location):\n",
    "    intermediate = []\n",
    "    all_lines = []\n",
    "    print('Extracting lines from ',len(files), 'files\\n')\n",
    "    \n",
    "    for filename in files:\n",
    "        intermediate += strip_tibet_chars(texts_dir+filename)\n",
    "    \n",
    "    all_lines = extract_text(intermediate)\n",
    "    print('Tokenizing ', len(all_lines),'lines\\n')\n",
    "    docs = make_tokenized_doc_list(all_lines)\n",
    "    print('Writing to file\\n')\n",
    "    write_tokenized_text_to_file(docs, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting lines from  28 files\n",
      "\n",
      "Tokenizing  52597 lines\n",
      "\n",
      "Writing to file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregate_all(files_list,save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_files(filename1, filename2, write_filename):\n",
    "    f1 = open(filename1,'r')\n",
    "    f2 = open(filename2,'r')\n",
    "    wf = open(write_filename,'w+')\n",
    "    line_num = 0\n",
    "    for line in f1:\n",
    "        line_num += 1\n",
    "        wf.write(line)\n",
    "        if((line_num % 10000000) == 0):\n",
    "            print(\"Wrote \",str(line_num),\"lines\\n\")\n",
    "    f1.close()\n",
    "    print(\"First file done!\\n\")\n",
    "    line_num = 0\n",
    "    for line in f2:\n",
    "        line_num += 1\n",
    "        wf.write(line)\n",
    "        if((line_num % 10000000) == 0):\n",
    "            print(\"Wrote \",str(line_num),\"lines\\n\")\n",
    "        wf.write(line)\n",
    "    print(\"Wrote \", str(line_num), \"lines from second file\\n\")\n",
    "    f2.close()\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote  10000000 lines\n",
      "\n",
      "Wrote  20000000 lines\n",
      "\n",
      "Wrote  30000000 lines\n",
      "\n",
      "Wrote  40000000 lines\n",
      "\n",
      "Wrote  50000000 lines\n",
      "\n",
      "Wrote  60000000 lines\n",
      "\n",
      "Wrote  70000000 lines\n",
      "\n",
      "Wrote  80000000 lines\n",
      "\n",
      "Wrote  90000000 lines\n",
      "\n",
      "Wrote  100000000 lines\n",
      "\n",
      "Wrote  110000000 lines\n",
      "\n",
      "Wrote  120000000 lines\n",
      "\n",
      "Wrote  130000000 lines\n",
      "\n",
      "First file done!\n",
      "\n",
      "Wrote  42903 lines from second file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename1 = \"/home/johannes/hypernym_discovery_data/UMBC_tokenized.txt\"\n",
    "filename2 = \"/home/johannes/thesis_code/data_experimentation/volvo_data/tokenized_sentences.txt\"\n",
    "filename3 = \"/home/johannes/hypernym_discovery_data/data_for_volvo_run/UMBC_volvo_tokenized.txt\"\n",
    "concatenate_files(filename1,filename2,filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testboi = strip_tibet_chars(texts_dir+files_list[0])\n",
    "texts_list = extract_text(testboi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584\n"
     ]
    }
   ],
   "source": [
    "print(testboi[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302\n"
     ]
    }
   ],
   "source": [
    "print(len(texts_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "༒༒༒༒༒Bogie\n",
      "\n",
      "_________\n",
      "Bogie\n",
      "############\n",
      "\n",
      "Variant symbols\n",
      "\n",
      "_________\n",
      "Variant symbols\n",
      "############\n",
      "\n",
      "SubChapter: ((RADD-GR or RADT-GR or RAPD-GR or RADDT-GR or RAPDD-GR or RADD-G2 or RADDT-G2 or RAPDT-GR or RAPD-G4)) AND\n",
      "\n",
      "_________\n",
      "SubChapter: ((RADD-GR or RADT-GR or RAPD-GR or RADDT-GR or RAPDD-GR or RADD-G2 or RADDT-G2 or RAPDT-GR or RAPD-G4)) AND\n",
      "############\n",
      "\n",
      "Section: (RSS-AIR) AND\n",
      "\n",
      "_________\n",
      "Section: (RSS-AIR) AND\n",
      "############\n",
      "\n",
      "Topic: (((RADD-GR or RADT-GR or RAPD-GR or RADDT-GR or RAPDD-GR or RADD-G2 or RADDT-G2 or RAPDT-GR or RAPD-G4) and RSS-AIR)) AND\n",
      "\n",
      "_________\n",
      "Topic: (((RADD-GR or RADT-GR or RAPD-GR or RADDT-GR or RAPDD-GR or RADD-G2 or RADDT-G2 or RAPDT-GR or RAPD-G4) and RSS-AIR)) AND\n",
      "############\n",
      "\n",
      "Sub-Section: (RAPDT-GR)\n",
      "\n",
      "_________\n",
      "Sub-Section: (RAPDT-GR)\n",
      "############\n",
      "\n",
      "Axle load distribution\n",
      "\n",
      "_________\n",
      "Axle load distribution\n",
      "############\n",
      "\n",
      "To obtain optimum traction the air suspension system can redistribute the weight between the driven axle and other axles. How much extra weight is put on the driven axle is governed mainly by the position of the bogie buttons and in some cases the speed.\n",
      "\n",
      "_________\n",
      "To obtain optimum traction the air suspension system can redistribute the weight between the driven axle and other axles. How much extra weight is put on the driven axle is governed mainly by the position of the bogie buttons and in some cases the speed.\n",
      "############\n",
      "\n",
      "Optimised traction\n",
      "\n",
      "_________\n",
      "Optimised traction\n",
      "############\n",
      "\n",
      "Normally, with a part load, the axle  weight is distributed between the axles in order to obtain a better grip on the road. See the diagram. The actual weight values depend on the truck's specification.\n",
      "\n",
      "_________\n",
      "Normally, with a part load, the axle  weight is distributed between the axles in order to obtain a better grip on the road. See the diagram. The actual weight values depend on the truck's specification.\n",
      "############\n",
      "\n",
      "584\n",
      "\n",
      "_________\n",
      "584\n",
      "############\n",
      "\n",
      "\u000e\n",
      "\n",
      "_________\n",
      "X PC24_FMFH_complete_with_variants_w1837\n",
      "############\n",
      "\n",
      "X PC24_FMFH_complete_with_variants_w1837\n",
      "\n",
      "_________\n",
      "Variant symbols\n",
      "############\n",
      "\n",
      "༒\n",
      "\n",
      "_________\n",
      "SubChapter: ((RADD-GR or RADT-GR or RAPD-GR or RADDT-GR or RAPDD-GR or RADD-G2 or RADDT-G2 or RAPDT-GR or RAPD-G4))\n",
      "############\n",
      "\n",
      "Variant symbols\n",
      "\n",
      "_________\n",
      "Section: (RSS-AIR)\n",
      "############\n",
      "\n",
      "SubChapter: ((RADD-GR or RADT-GR or RAPD-GR or RADDT-GR or RAPDD-GR or RADD-G2 or RADDT-G2 or RAPDT-GR or RAPD-G4))\n",
      "\n",
      "_________\n",
      "Topic: (((RADD-GR or RADT-GR or RAPD-GR or RADDT-GR or RAPDD-GR or RADD-G2 or RADDT-G2 or RAPDT-GR or RAPD-G4) and RSS-AIR))\n",
      "############\n",
      "\n",
      "Section: (RSS-AIR)\n",
      "\n",
      "_________\n",
      "Sub-Section: (RAPDT-GR)\n",
      "############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_num = 17\n",
    "for i in range(print_num):\n",
    "    print(testboi[i])\n",
    "    print(\"\\n_________\")\n",
    "    print(texts_list[i])\n",
    "    print(\"############\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x0e'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\u000e'.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
